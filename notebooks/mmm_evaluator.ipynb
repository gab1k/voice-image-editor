{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix9B7_NYudtx",
        "outputId": "d7b02958-5411-4355-878b-c9379a716c5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q torch torchvision torchaudio transformers\n",
        "!pip install -q openai-whisper pillow opencv-python scikit-image\n",
        "\n",
        "!pip install -q git+https://github.com/facebookresearch/ImageBind.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchcodec\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlbBhkMe1EIV",
        "outputId": "a35d8925-fc69-42b6-e01f-155a709a5fcd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchcodec in /usr/local/lib/python3.12/dist-packages (0.9.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import warnings\n",
        "import torchcodec\n",
        "from imagebind import data\n",
        "from imagebind.models import imagebind_model\n",
        "from imagebind.models.imagebind_model import ModalityType\n",
        "import tempfile\n",
        "import whisper\n",
        "import cv2\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "7HOvBog90Cvc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageBindEvaluator:\n",
        "\n",
        "    def __init__(self, device=None):\n",
        "        print(\"Loading ImageBind\")\n",
        "\n",
        "        try:\n",
        "\n",
        "            self.data = data\n",
        "            self.ModalityType = ModalityType\n",
        "\n",
        "            if device is None:\n",
        "                device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "            self.device = device\n",
        "\n",
        "            self.model = imagebind_model.imagebind_huge(pretrained=True)\n",
        "            self.model.eval()\n",
        "            self.model.to(self.device)\n",
        "\n",
        "            print(f\"ImageBind loaded on {self.device}\")\n",
        "\n",
        "            if self.device == 'cuda':\n",
        "                mem = torch.cuda.memory_allocated() / 1024**3\n",
        "                print(f\"   GPU Memory: {mem:.2f} GB\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" ImageBind loading failed: {e}\")\n",
        "\n",
        "            raise\n",
        "\n",
        "    def compute_audio_image_similarity(\n",
        "        self,\n",
        "        audio_path: str,\n",
        "        image: Image.Image\n",
        "    ) -> float:\n",
        "\n",
        "        with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as tmp:\n",
        "            image.save(tmp.name)\n",
        "            img_path = tmp.name\n",
        "\n",
        "        try:\n",
        "            inputs = {\n",
        "                self.ModalityType.AUDIO: self.data.load_and_transform_audio_data(\n",
        "                    [audio_path], self.device\n",
        "                ),\n",
        "                self.ModalityType.VISION: self.data.load_and_transform_vision_data(\n",
        "                    [img_path], self.device\n",
        "                ),\n",
        "            }\n",
        "\n",
        "\n",
        "            with torch.no_grad():\n",
        "                embeddings = self.model(inputs)\n",
        "\n",
        "\n",
        "            audio_emb = embeddings[self.ModalityType.AUDIO]\n",
        "            vision_emb = embeddings[self.ModalityType.VISION]\n",
        "\n",
        "            audio_emb = audio_emb / audio_emb.norm(dim=-1, keepdim=True)\n",
        "            vision_emb = vision_emb / vision_emb.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            # Cosine similarity\n",
        "            similarity = (audio_emb @ vision_emb.T).squeeze().item()\n",
        "\n",
        "\n",
        "            if self.device == 'cuda':\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "            normalized = (similarity + 0.3) / 0.8\n",
        "            normalized = max(0.0, min(1.0, normalized))\n",
        "            print(float(normalized))\n",
        "            return float(normalized)\n",
        "\n",
        "        finally:\n",
        "\n",
        "            import os\n",
        "            os.unlink(img_path)\n",
        "\n",
        "\n",
        "class WhisperCLIPEvaluator:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        whisper_model_size: str = 'base',\n",
        "        clip_model_name: str = 'openai/clip-vit-base-patch32',\n",
        "        device: str = None\n",
        "    ):\n",
        "\n",
        "        if device is None:\n",
        "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.device = device\n",
        "\n",
        "        print(\"Loading Whisper + CLIP...\")\n",
        "        self.whisper_model = whisper.load_model(whisper_model_size, device=device)\n",
        "        print(\"Whisper ready\")\n",
        "\n",
        "        self.clip_model = CLIPModel.from_pretrained(clip_model_name).to(device)\n",
        "        self.clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
        "        self.clip_model.eval()\n",
        "        print(\" CLIP ready\")\n",
        "\n",
        "\n",
        "        if device == 'cuda':\n",
        "            mem = torch.cuda.memory_allocated() / 1024**3\n",
        "\n",
        "    def transcribe_audio(self, audio_path: str) -> str:\n",
        "        result = self.whisper_model.transcribe(\n",
        "            audio_path,\n",
        "            language='ru',\n",
        "            fp16=(self.device == 'cuda')\n",
        "        )\n",
        "        return result[\"text\"].strip()\n",
        "\n",
        "    def compute_text_image_similarity(\n",
        "        self,\n",
        "        text: str,\n",
        "        image: Image.Image\n",
        "    ) -> float:\n",
        "\n",
        "        inputs = self.clip_processor(\n",
        "            text=[text],\n",
        "            images=[image],\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Embeddings\n",
        "        with torch.no_grad():\n",
        "            outputs = self.clip_model(**inputs)\n",
        "\n",
        "            text_embeds = outputs.text_embeds\n",
        "            image_embeds = outputs.image_embeds\n",
        "\n",
        "            # Normalize\n",
        "            text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n",
        "            image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            # Cosine similarity\n",
        "            similarity = (text_embeds @ image_embeds.T).squeeze().item()\n",
        "\n",
        "\n",
        "        if self.device == 'cuda':\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        return float(similarity)\n",
        "\n",
        "    def compute_audio_image_similarity(\n",
        "        self,\n",
        "        audio_path: str,\n",
        "        image: Image.Image,\n",
        "        verbose: bool = False\n",
        "    ) -> Tuple[float, str]:\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Transcribing audio...\", end=\" \")\n",
        "\n",
        "        text = self.transcribe_audio(audio_path)\n",
        "\n",
        "        if verbose:\n",
        "            print(f'\"{text}\"')\n",
        "\n",
        "        similarity = self.compute_text_image_similarity(text, image)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"{similarity:.4f}\")\n",
        "\n",
        "        return similarity, text\n",
        "\n",
        "class PerceptualQualityEvaluator:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def compute_sharpness(self, image: Image.Image) -> float:\n",
        "\n",
        "        # PIL -> grayscale numpy\n",
        "        img_array = np.array(image.convert('L')).astype(np.uint8)\n",
        "\n",
        "        # Laplacian\n",
        "        laplacian = cv2.Laplacian(img_array, cv2.CV_64F)\n",
        "        variance = laplacian.var()\n",
        "\n",
        "        # Normalizing\n",
        "        normalized = min(1.0, variance / 500.0)\n",
        "\n",
        "        return float(normalized)\n",
        "\n",
        "    def compute_contrast(self, image: Image.Image) -> float:\n",
        "        img_array = np.array(image.convert('L')).astype(np.float32)\n",
        "\n",
        "        # RMS contrast\n",
        "        contrast = img_array.std() / 128.0\n",
        "        contrast = min(1.0, contrast)\n",
        "\n",
        "        return float(contrast)\n",
        "\n",
        "    def compute_color_diversity(self, image: Image.Image) -> float:\n",
        "\n",
        "        if image.mode != 'RGB':\n",
        "            image = image.convert('RGB')\n",
        "\n",
        "        img_array = np.array(image).astype(np.float32)\n",
        "\n",
        "\n",
        "        std_per_channel = img_array.std(axis=(0, 1))\n",
        "        avg_std = std_per_channel.mean()\n",
        "\n",
        "        # Normalizing\n",
        "        normalized = min(1.0, avg_std / 80.0)\n",
        "\n",
        "        return float(normalized)\n",
        "\n",
        "    def compute_brightness(self, image: Image.Image) -> float:\n",
        "        img_array = np.array(image.convert('L')).astype(np.float32)\n",
        "        mean_brightness = img_array.mean()\n",
        "\n",
        "        deviation = abs(mean_brightness - 128) / 128.0\n",
        "        score = 1.0 - deviation\n",
        "\n",
        "        return float(max(0.0, score))\n",
        "\n",
        "    def evaluate(self, image: Image.Image) -> Dict[str, float]:\n",
        "\n",
        "        scores = {\n",
        "            'sharpness': self.compute_sharpness(image),\n",
        "            'contrast': self.compute_contrast(image),\n",
        "            'color_diversity': self.compute_color_diversity(image),\n",
        "            'brightness': self.compute_brightness(image)\n",
        "        }\n",
        "\n",
        "        # Combined quality score\n",
        "        scores['quality_score'] = (\n",
        "            scores['sharpness'] * 0.35 +\n",
        "            scores['contrast'] * 0.25 +\n",
        "            scores['color_diversity'] * 0.25 +\n",
        "            scores['brightness'] * 0.15\n",
        "        )\n",
        "\n",
        "        return scores\n",
        "\n",
        "\n",
        "class EditConsistencyEvaluator:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def compute_change_magnitude(\n",
        "        self,\n",
        "        original: Image.Image,\n",
        "        edited: Image.Image\n",
        "    ) -> float:\n",
        "\n",
        "        # Resize\n",
        "        if original.size != edited.size:\n",
        "            edited = edited.resize(original.size, Image.LANCZOS)\n",
        "\n",
        "        # Ensure RGB\n",
        "        if original.mode != 'RGB':\n",
        "            original = original.convert('RGB')\n",
        "        if edited.mode != 'RGB':\n",
        "            edited = edited.convert('RGB')\n",
        "\n",
        "        orig_array = np.array(original).astype(np.float32)\n",
        "        edit_array = np.array(edited).astype(np.float32)\n",
        "\n",
        "        # MSE\n",
        "        mse = np.mean((orig_array - edit_array) ** 2)\n",
        "\n",
        "        # Normalizing\n",
        "        normalized = min(1.0, mse / 10000.0)\n",
        "\n",
        "        return float(normalized)\n",
        "\n",
        "    def compute_structural_preservation(\n",
        "        self,\n",
        "        original: Image.Image,\n",
        "        edited: Image.Image\n",
        "    ) -> float:\n",
        "        from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "        if original.size != edited.size:\n",
        "            edited = edited.resize(original.size, Image.LANCZOS)\n",
        "\n",
        "        # Ensure RGB\n",
        "        if original.mode != 'RGB':\n",
        "            original = original.convert('RGB')\n",
        "        if edited.mode != 'RGB':\n",
        "            edited = edited.convert('RGB')\n",
        "\n",
        "        orig_array = np.array(original).astype(np.uint8)\n",
        "        edit_array = np.array(edited).astype(np.uint8)\n",
        "\n",
        "        try:\n",
        "            score = ssim(orig_array, edit_array, channel_axis=2, data_range=255)\n",
        "        except Exception as e:\n",
        "            print(f\" error: {e}, returning 0.5\")\n",
        "            score = 0.5\n",
        "\n",
        "        return float(score)\n",
        "\n",
        "    def evaluate(\n",
        "        self,\n",
        "        original: Image.Image,\n",
        "        edited: Image.Image\n",
        "    ) -> Dict[str, float]:\n",
        "        change_mag = self.compute_change_magnitude(original, edited)\n",
        "        struct_pres = self.compute_structural_preservation(original, edited)\n",
        "\n",
        "        change_penalty = 1.0 if change_mag > 0.1 else change_mag * 10\n",
        "\n",
        "        struct_penalty = struct_pres\n",
        "\n",
        "        consistency = (change_penalty * 0.3 + struct_penalty * 0.7)\n",
        "\n",
        "        return {\n",
        "            'change_magnitude': change_mag,\n",
        "            'structural_preservation': struct_pres,\n",
        "            'consistency_score': float(consistency)\n",
        "        }\n",
        "\n",
        "\n",
        "class BlindEvaluator:\n",
        "    def __init__(\n",
        "        self,\n",
        "        alignment_method: str = 'imagebind',  # 'imagebind' или 'whisper_clip'\n",
        "        weights: Optional[Dict[str, float]] = None,\n",
        "        device: str = None\n",
        "    ):\n",
        "\n",
        "        print(f\"Alignment method: {alignment_method}\")\n",
        "        print()\n",
        "\n",
        "        # Default weights\n",
        "        if weights is None:\n",
        "            weights = {\n",
        "                'alignment': 0.50,      # Главное - инструкция выполнена\n",
        "                'consistency': 0.35,    # Изменилось только нужное\n",
        "                'quality': 0.15        # Без явного брака\n",
        "            }\n",
        "        self.weights = weights\n",
        "\n",
        "\n",
        "        # Audio-Image Alignment\n",
        "        if alignment_method == 'imagebind':\n",
        "            try:\n",
        "                self.alignment_evaluator = ImageBindEvaluator(device=device)\n",
        "                self.alignment_method = 'imagebind'\n",
        "            except Exception as e:\n",
        "                print(f\"ImageBind failed, falling back to Whisper+CLIP\")\n",
        "                self.alignment_evaluator = WhisperCLIPEvaluator(device=device)\n",
        "                self.alignment_method = 'whisper_clip'\n",
        "        else:\n",
        "            self.alignment_evaluator = WhisperCLIPEvaluator(device=device)\n",
        "            self.alignment_method = 'whisper_clip'\n",
        "\n",
        "\n",
        "        self.quality_evaluator = PerceptualQualityEvaluator()\n",
        "        print(\"Quality Evaluator ready\")\n",
        "\n",
        "        # 3. Edit Consistency\n",
        "        self.consistency_evaluator = EditConsistencyEvaluator()\n",
        "        print(\"Consistency Evaluator ready\")\n",
        "\n",
        "\n",
        "        print(\" Evaluator initialized!\")\n",
        "\n",
        "\n",
        "    def evaluate_single(\n",
        "        self,\n",
        "        original_image: Image.Image,\n",
        "        edited_image: Image.Image,\n",
        "        audio_path: str,\n",
        "        verbose: bool = False\n",
        "    ) -> Dict[str, any]:\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Evaluating...\")\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        # 1. Audio-Image Alignment\n",
        "        if verbose:\n",
        "            print(\"  1. Audio-Image Alignment...\", end=\" \")\n",
        "\n",
        "        if self.alignment_method == 'imagebind':\n",
        "            alignment_score = self.alignment_evaluator.compute_audio_image_similarity(\n",
        "                audio_path, edited_image\n",
        "            )\n",
        "            results['alignment_score'] = alignment_score\n",
        "            results['transcribed_text'] = None\n",
        "        else:  # whisper_clip\n",
        "            alignment_score, transcribed = self.alignment_evaluator.compute_audio_image_similarity(\n",
        "                audio_path, edited_image, verbose=False\n",
        "            )\n",
        "            results['alignment_score'] = alignment_score\n",
        "            results['transcribed_text'] = transcribed\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"{alignment_score:.4f}\")\n",
        "            if results['transcribed_text']:\n",
        "                print(f\"Transcription: \\\"{results['transcribed_text']}\\\"\")\n",
        "\n",
        "        # 2. Perceptual Quality\n",
        "        if verbose:\n",
        "            print(\" 2. Perceptual Quality...\", end=\" \")\n",
        "\n",
        "        quality_scores = self.quality_evaluator.evaluate(edited_image)\n",
        "        results['quality_scores'] = quality_scores\n",
        "        results['quality_score'] = quality_scores['quality_score']\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"{results['quality_score']:.4f}\")\n",
        "\n",
        "        # 3. Edit Consistency\n",
        "        if verbose:\n",
        "            print(\"  3. Edit Consistency...\", end=\" \")\n",
        "\n",
        "        consistency_scores = self.consistency_evaluator.evaluate(\n",
        "            original_image, edited_image\n",
        "        )\n",
        "        results['consistency_scores'] = consistency_scores\n",
        "        results['consistency_score'] = consistency_scores['consistency_score']\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"{results['consistency_score']:.4f}\")\n",
        "\n",
        "        # Combined Score\n",
        "        combined = (\n",
        "            results['alignment_score'] * self.weights['alignment'] +\n",
        "            results['quality_score'] * self.weights['quality'] +\n",
        "            results['consistency_score'] * self.weights['consistency']\n",
        "        )\n",
        "\n",
        "        results['combined_score'] = float(combined)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\n  → Combined Score: {combined:.4f}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def evaluate_batch(\n",
        "        self,\n",
        "        original_image: Image.Image,\n",
        "        edited_images: List[Image.Image],\n",
        "        audio_path: str,\n",
        "        model_names: Optional[List[str]] = None,\n",
        "        verbose: bool = True\n",
        "    ) -> List[Dict[str, any]]:\n",
        "        if model_names is None:\n",
        "            model_names = [f\"Model_{i+1}\" for i in range(len(edited_images))]\n",
        "\n",
        "        results = []\n",
        "\n",
        "        for i, (edited_img, model_name) in enumerate(zip(edited_images, model_names)):\n",
        "            if verbose:\n",
        "                print(f\"\\n[{i+1}/{len(edited_images)}] {model_name}:\")\n",
        "\n",
        "            scores = self.evaluate_single(\n",
        "                original_image, edited_img, audio_path, verbose=verbose\n",
        "            )\n",
        "\n",
        "            scores['model_name'] = model_name\n",
        "            scores['model_index'] = i\n",
        "\n",
        "            results.append(scores)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def select_best(\n",
        "        self,\n",
        "        original_image: Image.Image,\n",
        "        edited_images: List[Image.Image],\n",
        "        audio_path: str,\n",
        "        model_names: Optional[List[str]] = None\n",
        "    ) -> Tuple[int, Dict[str, any]]:\n",
        "        results = self.evaluate_batch(\n",
        "            original_image, edited_images, audio_path, model_names\n",
        "        )\n",
        "\n",
        "        best_idx = max(range(len(results)),\n",
        "                      key=lambda i: results[i]['combined_score'])\n",
        "\n",
        "        return best_idx, results[best_idx]\n",
        "\n"
      ],
      "metadata": {
        "id": "LRvcg4eGup0e"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = BlindEvaluator(\n",
        "    weights={\n",
        "        'alignment': 0.40,\n",
        "        'consistency': 0.50,\n",
        "        'quality': 0.10\n",
        "    }\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5a94yrH1Xs0",
        "outputId": "6904c95e-4243-4be1-b56e-9da0b89c5fd8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alignment method: imagebind\n",
            "\n",
            "Loading ImageBind\n",
            "ImageBind loaded on cpu\n",
            "Quality Evaluator ready\n",
            "Consistency Evaluator ready\n",
            " Evaluator initialized!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "original = Image.open('/content/original.jpg')\n",
        "edited1 = Image.open('/content/ground_truth.jpg')\n",
        "edited2 = Image.open('/content/model_output.jpg')\n",
        "audio = '/content/test222.wav'\n",
        "\n",
        "results = evaluator.evaluate_batch(\n",
        "    original,\n",
        "    [edited1, edited2],\n",
        "    audio,\n",
        "    model_names=['Model1', 'Model2'],\n",
        "    verbose=True\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFewOleJvZAv",
        "outputId": "0a0e532e-5bde-45dd-e6fb-4229976a0ddf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[1/2] Model1:\n",
            "Evaluating...\n",
            "  1. Audio-Image Alignment... 0.276425614953041\n",
            "0.2764\n",
            " 2. Perceptual Quality... 0.7430\n",
            "  3. Edit Consistency... 0.6767\n",
            "\n",
            "  → Combined Score: 0.5232\n",
            "\n",
            "[2/2] Model2:\n",
            "Evaluating...\n",
            "  1. Audio-Image Alignment... 0.2990792170166969\n",
            "0.2991\n",
            " 2. Perceptual Quality... 0.9141\n",
            "  3. Edit Consistency... 0.7274\n",
            "\n",
            "  → Combined Score: 0.5748\n"
          ]
        }
      ]
    }
  ]
}