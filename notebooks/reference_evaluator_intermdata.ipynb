{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q lpips scikit-image opencv-python Pillow\n",
        "!pip install -q transformers torch torchvision torchcodec"
      ],
      "metadata": {
        "id": "E_QZVloKIvdc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchcodec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkp7PJRJ91MV",
        "outputId": "df4f323b-ad01-4fd2-86d3-d11b0ee657c1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchcodec in /usr/local/lib/python3.12/dist-packages (0.9.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import warnings\n",
        "import lpips\n",
        "import torchcodec\n",
        "from datasets import load_dataset\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#переиспользуем классы созданные в ноутбуке reference_evaluator.ipynb\n",
        "class LightweightLPIPS:\n",
        "    def __init__(self, net='alex', use_gpu=None):\n",
        "\n",
        "\n",
        "        if use_gpu is None:\n",
        "            use_gpu = torch.cuda.is_available()\n",
        "\n",
        "        self.device = 'cuda' if use_gpu else 'cpu'\n",
        "\n",
        "        print(f\"Loading LPIPS ({net})...\")\n",
        "\n",
        "\n",
        "        self.model = lpips.LPIPS(net=net, verbose=False)\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "        if self.device == 'cuda':\n",
        "            allocated = torch.cuda.memory_allocated() / 1024**2\n",
        "            print(f\"   GPU Memory: {allocated:.1f} MB\")\n",
        "\n",
        "    def compute(self, img1: Image.Image, img2: Image.Image) -> float:\n",
        "\n",
        "        tensor1 = self._preprocess(img1)\n",
        "        tensor2 = self._preprocess(img2)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            distance = self.model(tensor1, tensor2)\n",
        "\n",
        "        similarity = 1.0 - distance.item()\n",
        "\n",
        "\n",
        "        if self.device == 'cuda':\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        return max(0.0, min(1.0, similarity))\n",
        "\n",
        "    def _preprocess(self, img: Image.Image) -> torch.Tensor:\n",
        "\n",
        "        max_size = 512\n",
        "        if max(img.size) > max_size:\n",
        "            ratio = max_size / max(img.size)\n",
        "            new_size = tuple(int(dim * ratio) for dim in img.size)\n",
        "            img = img.resize(new_size, Image.LANCZOS)\n",
        "\n",
        "        img_array = np.array(img).astype(np.float32) / 255.0\n",
        "        tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)\n",
        "\n",
        "\n",
        "        tensor = tensor * 2 - 1\n",
        "\n",
        "        return tensor.to(self.device)\n",
        "\n",
        "    def __del__(self):\n",
        "\n",
        "        if hasattr(self, 'device') and self.device == 'cuda':\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "class ReferenceBasedEvaluator:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        use_lpips: bool = True,\n",
        "        use_ssim: bool = True,\n",
        "        use_clip: bool = False,  # По умолчанию выключен для экономии памяти\n",
        "        lpips_net: str = 'alex',\n",
        "        device: str = None\n",
        "    ):\n",
        "\n",
        "        if device is None:\n",
        "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "        print(\"Initializing Reference-Based Evaluator\")\n",
        "        print(f\"Device: {self.device}\")\n",
        "        print(f\"LPIPS: {use_lpips}\")\n",
        "        print(f\"SSIM: {use_ssim}\")\n",
        "        print(f\"CLIP: {use_clip}\")\n",
        "        print()\n",
        "\n",
        "        self.lpips_model = None\n",
        "        self.clip_model = None\n",
        "        self.use_ssim = use_ssim\n",
        "\n",
        "\n",
        "        if use_lpips:\n",
        "            try:\n",
        "                self.lpips_model = LightweightLPIPS(\n",
        "                    net=lpips_net,\n",
        "                    use_gpu=(self.device == 'cuda')\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\" LPIPS loading failed: {e}\")\n",
        "\n",
        "        if use_clip and self.device == 'cuda':\n",
        "            try:\n",
        "\n",
        "                from transformers import CLIPModel, CLIPProcessor\n",
        "\n",
        "                self.clip_model = CLIPModel.from_pretrained(\n",
        "                    \"openai/clip-vit-base-patch32\"\n",
        "                ).to(self.device)\n",
        "                self.clip_processor = CLIPProcessor.from_pretrained(\n",
        "                    \"openai/clip-vit-base-patch32\"\n",
        "                )\n",
        "                self.clip_model.eval()\n",
        "\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"CLIP loading failed: {e}\")\n",
        "                self.clip_model = None\n",
        "\n",
        "        print(\" Evaluator initialized\")\n",
        "\n",
        "\n",
        "    def compute_lpips(\n",
        "        self,\n",
        "        generated: Image.Image,\n",
        "        ground_truth: Image.Image\n",
        "    ) -> float:\n",
        "\n",
        "        if self.lpips_model is None:\n",
        "            return 0.0\n",
        "\n",
        "        try:\n",
        "            return self.lpips_model.compute(generated, ground_truth)\n",
        "        except Exception as e:\n",
        "            print(f\" LPIPS computation failed: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "\n",
        "    def compute_ssim(\n",
        "        self,\n",
        "        generated: Image.Image,\n",
        "        ground_truth: Image.Image\n",
        "    ) -> float:\n",
        "\n",
        "        if not self.use_ssim:\n",
        "            return 0.0\n",
        "\n",
        "        try:\n",
        "            from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "\n",
        "            gen_array = np.array(generated.convert('RGB'))\n",
        "            gt_array = np.array(ground_truth.convert('RGB'))\n",
        "\n",
        "\n",
        "            if gen_array.shape != gt_array.shape:\n",
        "                from skimage.transform import resize\n",
        "                gen_array = resize(gen_array, gt_array.shape, anti_aliasing=True)\n",
        "                gen_array = (gen_array * 255).astype(np.uint8)\n",
        "\n",
        "\n",
        "            score = ssim(\n",
        "                gen_array,\n",
        "                gt_array,\n",
        "                channel_axis=2,\n",
        "                data_range=255\n",
        "            )\n",
        "\n",
        "            return float(score)\n",
        "        except Exception as e:\n",
        "            print(f\"SSIM computation failed: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    def compute_psnr(\n",
        "        self,\n",
        "        generated: Image.Image,\n",
        "        ground_truth: Image.Image\n",
        "    ) -> float:\n",
        "\n",
        "        try:\n",
        "            gen_array = np.array(generated.convert('RGB')).astype(np.float64)\n",
        "            gt_array = np.array(ground_truth.convert('RGB')).astype(np.float64)\n",
        "\n",
        "\n",
        "            if gen_array.shape != gt_array.shape:\n",
        "                from skimage.transform import resize\n",
        "                gen_array = resize(gen_array, gt_array.shape, anti_aliasing=True)\n",
        "                gen_array = gen_array * 255\n",
        "\n",
        "            mse = np.mean((gen_array - gt_array) ** 2)\n",
        "\n",
        "            if mse == 0:\n",
        "                return 100.0\n",
        "\n",
        "            max_pixel = 255.0\n",
        "            psnr = 20 * np.log10(max_pixel / np.sqrt(mse))\n",
        "\n",
        "            return float(psnr)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" PSNR computation failed: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "\n",
        "    def compute_clip_similarity(\n",
        "        self,\n",
        "        generated: Image.Image,\n",
        "        ground_truth: Image.Image\n",
        "    ) -> float:\n",
        "\n",
        "        if self.clip_model is None:\n",
        "            return 0.0\n",
        "\n",
        "        try:\n",
        "            inputs = self.clip_processor(\n",
        "                images=[generated, ground_truth],\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                image_features = self.clip_model.get_image_features(**inputs)\n",
        "\n",
        "                # Normalize\n",
        "                image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "                # Cosine similarity\n",
        "                similarity = torch.cosine_similarity(\n",
        "                    image_features[0:1],\n",
        "                    image_features[1:2]\n",
        "                ).item()\n",
        "\n",
        "            # Cleanup\n",
        "            if self.device == 'cuda':\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            return float(similarity)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" CLIP computation failed: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "\n",
        "    def compute_color_similarity(\n",
        "        self,\n",
        "        generated: Image.Image,\n",
        "        ground_truth: Image.Image\n",
        "    ) -> float:\n",
        "\n",
        "        try:\n",
        "            import cv2\n",
        "\n",
        "            gen_array = np.array(generated.convert('RGB'))\n",
        "            gt_array = np.array(ground_truth.convert('RGB'))\n",
        "\n",
        "\n",
        "            if gen_array.shape != gt_array.shape:\n",
        "                gen_array = cv2.resize(gen_array, (gt_array.shape[1], gt_array.shape[0]))\n",
        "\n",
        "\n",
        "            hist_gen = cv2.calcHist(\n",
        "                [gen_array], [0, 1, 2], None,\n",
        "                [8, 8, 8], [0, 256, 0, 256, 0, 256]\n",
        "            )\n",
        "            hist_gt = cv2.calcHist(\n",
        "                [gt_array], [0, 1, 2], None,\n",
        "                [8, 8, 8], [0, 256, 0, 256, 0, 256]\n",
        "            )\n",
        "\n",
        "\n",
        "            hist_gen = cv2.normalize(hist_gen, hist_gen).flatten()\n",
        "            hist_gt = cv2.normalize(hist_gt, hist_gt).flatten()\n",
        "\n",
        "\n",
        "            correlation = cv2.compareHist(\n",
        "                hist_gen.reshape(-1, 1),\n",
        "                hist_gt.reshape(-1, 1),\n",
        "                cv2.HISTCMP_CORREL\n",
        "            )\n",
        "\n",
        "            return float(correlation)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Color similarity computation failed: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "\n",
        "\n",
        "    def evaluate_single(\n",
        "        self,\n",
        "        generated: Image.Image,\n",
        "        ground_truth: Image.Image,\n",
        "        weights: Optional[Dict[str, float]] = None,\n",
        "        verbose: bool = False\n",
        "    ) -> Dict[str, float]:\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Evaluating...\", end=\" \")\n",
        "\n",
        "        results = {}\n",
        "\n",
        "\n",
        "        if weights is None:\n",
        "            weights = {\n",
        "                'lpips': 0.50,\n",
        "                'ssim': 0.30,\n",
        "                'clip': 0.0,\n",
        "                'color': 0.20\n",
        "            }\n",
        "\n",
        "        results['lpips_similarity'] = self.compute_lpips(generated, ground_truth)\n",
        "\n",
        "\n",
        "        results['ssim'] = self.compute_ssim(generated, ground_truth)\n",
        "\n",
        "        results['psnr'] = self.compute_psnr(generated, ground_truth)\n",
        "\n",
        "\n",
        "        if self.clip_model is not None:\n",
        "            results['clip_similarity'] = self.compute_clip_similarity(\n",
        "                generated, ground_truth\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "        results['color_similarity'] = self.compute_color_similarity(\n",
        "            generated, ground_truth\n",
        "        )\n",
        "\n",
        "\n",
        "        combined = 0.0\n",
        "        total_weight = 0.0\n",
        "\n",
        "        for metric, weight in weights.items():\n",
        "            metric_key = f'{metric}_similarity' if metric != 'ssim' else metric\n",
        "\n",
        "            if metric_key in results:\n",
        "                value = results[metric_key]\n",
        "\n",
        "\n",
        "                if metric == 'psnr':\n",
        "                    value = min(1.0, max(0.0, (value - 20) / 20))\n",
        "\n",
        "                combined += value * weight\n",
        "                total_weight += weight\n",
        "\n",
        "        results['combined_score'] = combined / total_weight if total_weight > 0 else 0.0\n",
        "        return results\n"
      ],
      "metadata": {
        "id": "bJJkLnXqYGsW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Допишем класс для инференса датасета\n",
        "class LightweightReferenceEvaluator:\n",
        "    def __init__(\n",
        "        self,\n",
        "        use_lpips: bool = True,\n",
        "        use_ssim: bool = True,\n",
        "        use_psnr: bool = True,\n",
        "        lpips_net: str = 'alex',\n",
        "        device: str = None\n",
        "    ):\n",
        "        if device is None:\n",
        "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "        self.use_ssim = use_ssim\n",
        "        self.use_psnr = use_psnr\n",
        "        self.lpips_model = None\n",
        "\n",
        "        if use_lpips:\n",
        "            try:\n",
        "                self.lpips_model = lpips.LPIPS(net=lpips_net, verbose=False)\n",
        "                self.lpips_model = self.lpips_model.to(self.device)\n",
        "                self.lpips_model.eval()\n",
        "\n",
        "                for param in self.lpips_model.parameters():\n",
        "                    param.requires_grad = False\n",
        "\n",
        "                if self.device == 'cuda':\n",
        "                    mem = torch.cuda.memory_allocated() / 1024**2\n",
        "                    print(f\"  GPU Memory: {mem:.1f} MB\")\n",
        "            except Exception as e:\n",
        "                print(f\"LPIPS loading failed: {e}\")\n",
        "                self.lpips_model = None\n",
        "\n",
        "\n",
        "    def _to_tensor(self, img: Image.Image, max_size: int = 512) -> torch.Tensor:\n",
        "\n",
        "        if max(img.size) > max_size:\n",
        "            ratio = max_size / max(img.size)\n",
        "            new_size = tuple(int(dim * ratio) for dim in img.size)\n",
        "            img = img.resize(new_size, Image.LANCZOS)\n",
        "\n",
        "        if img.mode != 'RGB':\n",
        "            img = img.convert('RGB')\n",
        "\n",
        "        img_array = np.array(img).astype(np.float32) / 255.0\n",
        "        tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)\n",
        "        tensor = tensor * 2 - 1  # Normalize to [-1, 1]\n",
        "\n",
        "        return tensor.to(self.device)\n",
        "\n",
        "    def compute_lpips(self, generated: Image.Image, ground_truth: Image.Image) -> float:\n",
        "        if self.lpips_model is None:\n",
        "            return 0.0\n",
        "\n",
        "        try:\n",
        "            tensor1 = self._to_tensor(generated)\n",
        "            tensor2 = self._to_tensor(ground_truth)\n",
        "\n",
        "            if tensor1.shape != tensor2.shape:\n",
        "                tensor1 = torch.nn.functional.interpolate(\n",
        "                    tensor1, size=tensor2.shape[2:], mode='bilinear', align_corners=False\n",
        "                )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                distance = self.lpips_model(tensor1, tensor2)\n",
        "\n",
        "            similarity = 1.0 - distance.item()\n",
        "\n",
        "            if self.device == 'cuda':\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            return max(0.0, min(1.0, similarity))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" LPIPS error: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    def compute_ssim(self, generated: Image.Image, ground_truth: Image.Image) -> float:\n",
        "        if not self.use_ssim:\n",
        "            return 0.0\n",
        "\n",
        "        try:\n",
        "            from skimage.metrics import structural_similarity as ssim\n",
        "            from skimage.transform import resize\n",
        "\n",
        "            gen_array = np.array(generated.convert('RGB'))\n",
        "            gt_array = np.array(ground_truth.convert('RGB'))\n",
        "\n",
        "\n",
        "            if gen_array.shape != gt_array.shape:\n",
        "                gen_array = resize(\n",
        "                    gen_array, gt_array.shape,\n",
        "                    anti_aliasing=True, preserve_range=True\n",
        "                ).astype(np.uint8)\n",
        "\n",
        "            score = ssim(gen_array, gt_array, channel_axis=2, data_range=255)\n",
        "            return float(score)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" SSIM error: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    def compute_psnr(self, generated: Image.Image, ground_truth: Image.Image) -> float:\n",
        "\n",
        "        if not self.use_psnr:\n",
        "            return 0.0\n",
        "\n",
        "        try:\n",
        "            from skimage.transform import resize\n",
        "\n",
        "            gen_array = np.array(generated.convert('RGB')).astype(np.float64)\n",
        "            gt_array = np.array(ground_truth.convert('RGB')).astype(np.float64)\n",
        "\n",
        "\n",
        "            if gen_array.shape != gt_array.shape:\n",
        "                gen_array = resize(\n",
        "                    gen_array, gt_array.shape,\n",
        "                    anti_aliasing=True, preserve_range=True\n",
        "                )\n",
        "\n",
        "            mse = np.mean((gen_array - gt_array) ** 2)\n",
        "\n",
        "            if mse == 0:\n",
        "                return 100.0\n",
        "\n",
        "            psnr = 20 * np.log10(255.0 / np.sqrt(mse))\n",
        "            return float(psnr)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    def compute_mse(self, generated: Image.Image, ground_truth: Image.Image) -> float:\n",
        "        try:\n",
        "            from skimage.transform import resize\n",
        "\n",
        "            gen_array = np.array(generated.convert('RGB')).astype(np.float64)\n",
        "            gt_array = np.array(ground_truth.convert('RGB')).astype(np.float64)\n",
        "\n",
        "            if gen_array.shape != gt_array.shape:\n",
        "                gen_array = resize(\n",
        "                    gen_array, gt_array.shape,\n",
        "                    anti_aliasing=True, preserve_range=True\n",
        "                )\n",
        "\n",
        "            mse = np.mean((gen_array - gt_array) ** 2)\n",
        "            normalized_mse = mse / (255.0 ** 2)\n",
        "\n",
        "            return float(normalized_mse)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"MSE error: {e}\")\n",
        "            return 1.0\n",
        "\n",
        "    def evaluate_single(\n",
        "        self,\n",
        "        generated: Image.Image,\n",
        "        ground_truth: Image.Image,\n",
        "        verbose: bool = False\n",
        "    ) -> Dict[str, float]:\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        results['lpips_similarity'] = self.compute_lpips(generated, ground_truth)\n",
        "        results['ssim'] = self.compute_ssim(generated, ground_truth)\n",
        "\n",
        "        results['psnr'] = self.compute_psnr(generated, ground_truth)\n",
        "\n",
        "        results['mse'] = self.compute_mse(generated, ground_truth)\n",
        "        psnr_normalized = min(1.0, max(0.0, (results['psnr'] - 15) / 25))\n",
        "\n",
        "        results['combined_score'] = (\n",
        "            results['lpips_similarity'] * 0.4 +\n",
        "            results['ssim'] * 0.4 +\n",
        "            psnr_normalized * 0.2\n",
        "        )\n",
        "\n",
        "\n",
        "        return results\n",
        "\n",
        "    def cleanup(self):\n",
        "        if self.lpips_model is not None:\n",
        "            del self.lpips_model\n",
        "            self.lpips_model = None\n",
        "\n",
        "        if self.device == 'cuda':\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "def evaluate_with_reference(\n",
        "    result_dataset: str,\n",
        "    ground_truth_dataset: str,\n",
        "    result_image_col: str = \"result_image\",\n",
        "    ground_truth_col: str = \"OUTPUT_IMG\",\n",
        "    id_col: str = \"IMAGE_ID\",\n",
        "    load_samples: Optional[int] = None,\n",
        "    max_samples: Optional[int] = None,\n",
        "    save_path: Optional[str] = None,\n",
        "    save_every: int = 10,\n",
        "    resume_from: Optional[str] = None,\n",
        "    use_lpips: bool = True,\n",
        "    use_ssim: bool = True,\n",
        "    use_psnr: bool = True\n",
        ") -> pd.DataFrame:\n",
        "\n",
        "    evaluator = LightweightReferenceEvaluator(\n",
        "        use_lpips=use_lpips,\n",
        "        use_ssim=use_ssim,\n",
        "        use_psnr=use_psnr\n",
        "    )\n",
        "\n",
        "\n",
        "    processed_ids = set()\n",
        "    results = []\n",
        "\n",
        "    if resume_from and Path(resume_from).exists():\n",
        "        existing_df = pd.read_csv(resume_from)\n",
        "        results = existing_df.to_dict('records')\n",
        "        processed_ids = set(existing_df['IMAGE_ID'].tolist())\n",
        "\n",
        "    gt_stream = load_dataset(ground_truth_dataset, split='train', streaming=True)\n",
        "\n",
        "    gt_dict = {}\n",
        "    for idx, item in enumerate(gt_stream):\n",
        "        image_id = (item.get(id_col) or item.get('IMAGE_ID') or\n",
        "                    item.get('image_id') or item.get('id'))\n",
        "        if image_id is not None:\n",
        "            gt_dict[image_id] = item\n",
        "        if load_samples and idx >= load_samples - 1:\n",
        "            break\n",
        "\n",
        "    if gt_dict:\n",
        "        sample_gt = list(gt_dict.values())[0]\n",
        "\n",
        "    result_stream = load_dataset(result_dataset, split='train', streaming=True)\n",
        "\n",
        "\n",
        "    processed_count = 0\n",
        "    skipped_count = 0\n",
        "\n",
        "    for idx, result_item in enumerate(tqdm(result_stream, desc=\"Evaluating\")):\n",
        "\n",
        "        if load_samples and idx >= load_samples:\n",
        "            break\n",
        "\n",
        "\n",
        "        if max_samples and processed_count >= max_samples:\n",
        "            break\n",
        "\n",
        "\n",
        "        image_id = (result_item.get(id_col) or result_item.get('IMAGE_ID') or\n",
        "                    result_item.get('image_id') or result_item.get('id'))\n",
        "\n",
        "        if image_id is None:\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "\n",
        "        if image_id in processed_ids:\n",
        "            continue\n",
        "\n",
        "        if image_id not in gt_dict:\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "\n",
        "        gt_item = gt_dict[image_id]\n",
        "\n",
        "        try:\n",
        "            result_img = _get_image(result_item, result_image_col)\n",
        "            gt_img = _get_image(gt_item, ground_truth_col)\n",
        "\n",
        "            if result_img is None or gt_img is None:\n",
        "                skipped_count += 1\n",
        "                continue\n",
        "\n",
        "            scores = evaluator.evaluate_single(result_img, gt_img)\n",
        "\n",
        "            result_entry = {\n",
        "                'IMAGE_ID': image_id,\n",
        "                'lpips_similarity': scores['lpips_similarity'],\n",
        "                'ssim': scores['ssim'],\n",
        "                'psnr': scores['psnr'],\n",
        "                'mse': scores['mse'],\n",
        "                'combined_score': scores['combined_score'],\n",
        "                'error': ''\n",
        "            }\n",
        "            results.append(result_entry)\n",
        "            processed_count += 1\n",
        "\n",
        "            if save_path and processed_count % save_every == 0:\n",
        "                pd.DataFrame(results).to_csv(save_path, index=False)\n",
        "\n",
        "\n",
        "            if processed_count % 20 == 0:\n",
        "                gc.collect()\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "        except Exception as e:\n",
        "            tqdm.write(f\"   {image_id}: {e}\")\n",
        "            results.append({\n",
        "                'IMAGE_ID': image_id,\n",
        "                'lpips_similarity': 0.0,\n",
        "                'ssim': 0.0,\n",
        "                'psnr': 0.0,\n",
        "                'mse': 1.0,\n",
        "                'combined_score': 0.0,\n",
        "                'error': str(e)\n",
        "            })\n",
        "\n",
        "\n",
        "    evaluator.cleanup()\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    # Сохраняем\n",
        "    if save_path:\n",
        "        results_df.to_csv(save_path, index=False)\n",
        "        print(f\"{save_path}\")\n",
        "\n",
        "\n",
        "    return results_df\n",
        "\n",
        "\n",
        "def _get_image(item: dict, col_name: str) -> Optional[Image.Image]:\n",
        "    img = (item.get(col_name) or item.get(col_name.lower()) or\n",
        "           item.get('image') or item.get('img'))\n",
        "\n",
        "    if img is None:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        if isinstance(img, Image.Image):\n",
        "            return img.convert('RGB')\n",
        "\n",
        "        if isinstance(img, dict):\n",
        "            if 'bytes' in img:\n",
        "                return Image.open(io.BytesIO(img['bytes'])).convert('RGB')\n",
        "            if 'path' in img:\n",
        "                return Image.open(img['path']).convert('RGB')\n",
        "\n",
        "        if isinstance(img, bytes):\n",
        "            return Image.open(io.BytesIO(img)).convert('RGB')\n",
        "\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"mage loading error: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def compare_models_with_reference(\n",
        "    result_datasets: Dict[str, str],\n",
        "    ground_truth_dataset: str,\n",
        "    ground_truth_col: str = \"OUTPUT_IMG\",\n",
        "    **kwargs\n",
        ") -> pd.DataFrame:\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    for model_name, result_dataset in result_datasets.items():\n",
        "\n",
        "        df = evaluate_with_reference(\n",
        "            result_dataset=result_dataset,\n",
        "            ground_truth_dataset=ground_truth_dataset,\n",
        "            ground_truth_col=ground_truth_col,\n",
        "            save_path=f\"ref_results_{model_name}.csv\",\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "        df['model'] = model_name\n",
        "        all_results.append(df)\n",
        "\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    combined_df = pd.concat(all_results, ignore_index=True)\n",
        "\n",
        "    summary = combined_df.groupby('model').agg({\n",
        "        'lpips_similarity': ['mean', 'std'],\n",
        "        'ssim': ['mean', 'std'],\n",
        "        'psnr': ['mean', 'std'],\n",
        "        'combined_score': ['mean', 'std']\n",
        "    }).round(4)\n",
        "\n",
        "    print(summary)\n",
        "\n",
        "    return combined_df\n",
        "\n"
      ],
      "metadata": {
        "id": "VNFnzeep9HRj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchcodec\n",
        "results = evaluate_with_reference(\n",
        "  result_dataset=\"gab1k/mmm_project_with_ru_intermdata\",\n",
        "  ground_truth_dataset=\"arood0/mmm_project_with_audio_ru_final\",\n",
        "  result_image_col=\"result_image\",\n",
        "  ground_truth_col=\"OUTPUT_IMG\",\n",
        "  load_samples=300,\n",
        "  max_samples=10,\n",
        "  save_path=\"reference_eval_results.csv\",\n",
        "  save_every=5\n",
        ")\n",
        "\n",
        "print(\"\\nResults preview:\")\n",
        "print(results[['IMAGE_ID', 'lpips_similarity', 'ssim', 'psnr', 'combined_score']].head(10))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7iF5LI186hZ",
        "outputId": "3932626d-8611-40f5-9717-f0b9b6778d4f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 10it [00:20,  2.06s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reference_eval_results.csv\n",
            "\n",
            "Results preview:\n",
            "      IMAGE_ID  lpips_similarity      ssim       psnr  combined_score\n",
            "0  BEwrVP6o0yQ          0.753962  0.461809  17.756203        0.508358\n",
            "1  BdH23PovaTA          0.752320  0.823275  14.035430        0.630238\n",
            "2  bFdWsiXblVw          0.871372  0.775359  22.478411        0.718520\n",
            "3  bfF-9S0ktP8          0.801408  0.848465  23.605118        0.728790\n",
            "4  bfWJOx132As          0.906186  0.872151  21.662621        0.764636\n",
            "5  BfO20utCi0I          0.549404  0.497361  14.688648        0.418706\n",
            "6  Bfp634LE8Cc          0.731500  0.583868  19.044910        0.558506\n",
            "7  Bg0Geue-cY8          0.832323  0.809618  17.144528        0.673933\n",
            "8  Bgae-sqbe_g          0.571017  0.608394  11.402190        0.471764\n",
            "9  bgDSZ-w2gIM          0.676759  0.748714  11.863627        0.570189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"lpips_similarity\", results['lpips_similarity'].mean())\n",
        "print(\"ssim\", results['ssim'].mean())\n",
        "print(\"psnr\", results['psnr'].mean())\n",
        "print(\"combined_score\", results['combined_score'].mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYYv0Sn6BXGo",
        "outputId": "362db0b9-3f16-4e31-95c1-d987ce1121eb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lpips_similarity 0.7446252033114433\n",
            "ssim 0.702901249933613\n",
            "psnr 17.36816850752275\n",
            "combined_score 0.6043640141488593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Хочется сделать какой-то вывод, но пока что рано, тк нет других моделей с которыми мы можем сравниться и сделать вывод кто лучше)\n"
      ],
      "metadata": {
        "id": "rfEZAckVBtrh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "en4trMGMB14l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}