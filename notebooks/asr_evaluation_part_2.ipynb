{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asr evaluation part 2\n",
    "\n",
    "В этом ноутбуке протестим некоторые модели на датасете русских голосовых команд для редактирования изображений.\n",
    "\n",
    "Датасет: ```arood0/mmm_project_with_audio_ru_final```\n",
    "\n",
    "Модели:\n",
    "- Whisper\n",
    "- GigaAM-v3\n",
    "- T-one\n",
    "- NVIDIA Parakeet-TDT-0.6B-v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-12-07 07:03:34 nemo_logging:405] Megatron num_microbatches_calculator not found, using Apex version.\n",
      "OneLogger: Setting error_handling_strategy to DISABLE_QUIETLY_AND_REPORT_METRIC_ERROR for rank (rank=0) with OneLogger disabled. To override: explicitly set error_handling_strategy parameter.\n",
      "No exporters were provided. This means that no telemetry data will be collected.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, AutoModel\n",
    "from pathlib import Path\n",
    "import soundfile as sf\n",
    "import sys\n",
    "from datasets import load_dataset, Audio\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from jiwer import wer, cer\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['IMAGE_ID', 'EDITING_TYPE', 'CORE', 'MASK', 'EDITING_INSTRUCTION', 'OUTPUT_DESCRIPTION', 'INPUT_CAPTION_BY_LLAMA', 'OUTPUT_CAPTION_BY_LLAMA', 'INPUT_IMG', 'MASK_IMG', 'OUTPUT_IMG', 'EDITING_INSTRUCTION_RU', 'audio'],\n",
       "    num_rows: 2000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"arood0/mmm_project_with_audio_ru_final\")\n",
    "\n",
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 2000\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = dataset['train']\n",
    "eval_dataset = eval_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "print(f\"Dataset size: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_CACHE_DIR = Path(\"/home/sallundina/voice-image-editor/notebooks/audio_cache\")\n",
    "AUDIO_CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def download_audio_from_sample(sample, sample_idx):\n",
    "    audio_file = AUDIO_CACHE_DIR / f\"sample_{sample_idx}.wav\"\n",
    "    \n",
    "    if audio_file.exists():\n",
    "        return str(audio_file)\n",
    "    \n",
    "    audio_obj = sample['audio']\n",
    "    \n",
    "    if hasattr(audio_obj, 'get_all_samples'):\n",
    "        try:\n",
    "            samples = audio_obj.get_all_samples()\n",
    "            if hasattr(samples, 'data'):\n",
    "                audio_array = np.array(samples.data, dtype=np.float32)\n",
    "                \n",
    "            sr = 16000\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error getting samples: {e}\")\n",
    "    \n",
    "    audio_array = np.asarray(audio_array, dtype=np.float32)\n",
    "    \n",
    "    if len(audio_array.shape) > 1:\n",
    "        audio_array = audio_array.flatten()\n",
    "    \n",
    "    if len(audio_array) == 0:\n",
    "        raise ValueError(f\"Empty audio array for sample {sample_idx}\")\n",
    "    \n",
    "    max_val = np.abs(audio_array).max()\n",
    "    if max_val > 1.0:\n",
    "        audio_array = audio_array / max_val\n",
    "    \n",
    "    sf.write(str(audio_file), audio_array, int(sr), subtype='PCM_16', format='WAV')\n",
    "    return str(audio_file)\n",
    "\n",
    "for i in tqdm(range(len(eval_dataset)), desc=\"Downloading audio\"):\n",
    "    sample = eval_dataset[i]\n",
    "    download_audio_from_sample(sample, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_whisper(audio_input, processor=None, model=None, sr=None, language='ru'):\n",
    "    if processor is None:\n",
    "        processor = processor\n",
    "    if model is None:\n",
    "        model = model\n",
    "    \n",
    "    WHISPER_SR = 16000\n",
    "    \n",
    "    audio_array = librosa.load(audio_input, sr=WHISPER_SR, mono=True)[0]\n",
    "    sr = WHISPER_SR\n",
    "    \n",
    "    input_features = processor(audio_array, sampling_rate=sr, return_tensors=\"pt\").input_features.to(device)\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        input_features,\n",
    "        language=language,\n",
    "        task=\"transcribe\"\n",
    "    )\n",
    "    \n",
    "    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return transcription.lower()\n",
    "\n",
    "def create_whisper_transcriber(model_name):\n",
    "    processor = WhisperProcessor.from_pretrained(model_name)\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "    \n",
    "    def transcribe_func(audio_input):\n",
    "        return transcribe_whisper(audio_input, processor=processor, model=model)\n",
    "    \n",
    "    return transcribe_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GigaAM-v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "revision = \"e2e_rnnt\"\n",
    "gigaam_model = AutoModel.from_pretrained(\n",
    "    \"ai-sage/GigaAM-v3\",\n",
    "    revision=revision,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_gigaam(audio_input):\n",
    "    try:\n",
    "        transcription = gigaam_model.transcribe(audio_input)\n",
    "    except ValueError as e:\n",
    "        if \"Too long\" in str(e):\n",
    "            transcription = gigaam_model.transcribe_longform(audio_input)\n",
    "        else:\n",
    "            raise\n",
    "    return transcription.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tone_repo_path = Path(\"/tmp/T-one\")\n",
    "if not tone_repo_path.exists():\n",
    "    import subprocess\n",
    "    subprocess.run([\"git\", \"clone\", \"https://github.com/voicekit-team/T-one.git\", str(tone_repo_path)], check=True)\n",
    "\n",
    "if str(tone_repo_path) not in sys.path:\n",
    "    sys.path.insert(0, str(tone_repo_path))\n",
    "\n",
    "from tone import StreamingCTCPipeline, read_audio\n",
    "\n",
    "tone_pipeline = StreamingCTCPipeline.from_hugging_face()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_tone(audio_input):\n",
    "    audio = read_audio(audio_input)\n",
    "    phrases = tone_pipeline.forward_offline(audio)\n",
    "    transcription = \" \".join([p.text for p in phrases]).strip()\n",
    "    return transcription.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NVIDIA Parakeet-TDT-0.6B-v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-12-07 07:03:50 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 8192 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-12-07 07:03:55 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    use_lhotse: true\n",
      "    skip_missing_manifest_entries: true\n",
      "    input_cfg: null\n",
      "    tarred_audio_filepaths: null\n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    shuffle: true\n",
      "    num_workers: 2\n",
      "    pin_memory: true\n",
      "    max_duration: 10.0\n",
      "    min_duration: 1.0\n",
      "    text_field: answer\n",
      "    batch_duration: null\n",
      "    max_tps: null\n",
      "    use_bucketing: true\n",
      "    bucket_duration_bins: null\n",
      "    bucket_batch_size: null\n",
      "    num_buckets: 30\n",
      "    bucket_buffer_size: 20000\n",
      "    shuffle_buffer_size: 10000\n",
      "    \n",
      "[NeMo W 2025-12-07 07:03:55 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    use_lhotse: true\n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    max_duration: 40.0\n",
      "    min_duration: 0.1\n",
      "    num_workers: 2\n",
      "    pin_memory: true\n",
      "    text_field: answer\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-12-07 07:03:55 nemo_logging:393] PADDING: 0\n",
      "[NeMo I 2025-12-07 07:04:00 nemo_logging:393] Using RNNT Loss : tdt\n",
      "    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n",
      "[NeMo I 2025-12-07 07:04:00 nemo_logging:393] Using RNNT Loss : tdt\n",
      "    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n",
      "[NeMo I 2025-12-07 07:04:00 nemo_logging:393] Using RNNT Loss : tdt\n",
      "    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n",
      "[NeMo I 2025-12-07 07:04:03 nemo_logging:393] Model EncDecRNNTBPEModel was successfully restored from /home/sallundina/.cache/huggingface/hub/models--nvidia--parakeet-tdt-0.6b-v3/snapshots/6d590f77001d318fb17a0b5bf7ee329a91b52598/parakeet-tdt-0.6b-v3.nemo.\n"
     ]
    }
   ],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "\n",
    "parakeet_model = nemo_asr.models.ASRModel.from_pretrained(model_name=\"nvidia/parakeet-tdt-0.6b-v3\")\n",
    "\n",
    "parakeet_model.eval()\n",
    "\n",
    "if hasattr(parakeet_model, 'decoding'):\n",
    "    if hasattr(parakeet_model.decoding, 'decoding_computer'):\n",
    "        decoding_computer = parakeet_model.decoding.decoding_computer\n",
    "        if hasattr(decoding_computer, 'cuda_graphs_mode'):\n",
    "            decoding_computer.cuda_graphs_mode = None\n",
    "        if hasattr(parakeet_model.decoding, 'cfg') and hasattr(parakeet_model.decoding.cfg, 'cuda_graphs_mode'):\n",
    "            parakeet_model.decoding.cfg.cuda_graphs_mode = None\n",
    "\n",
    "parakeet_model_cpu = parakeet_model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_parakeet(audio_input):\n",
    "    if hasattr(parakeet_model_cpu, 'decoding') and hasattr(parakeet_model_cpu.decoding, 'decoding_computer'):\n",
    "        if hasattr(parakeet_model_cpu.decoding.decoding_computer, 'cuda_graphs_mode'):\n",
    "            parakeet_model_cpu.decoding.decoding_computer.cuda_graphs_mode = None\n",
    "            \n",
    "    outputs = parakeet_model.transcribe([audio_input], batch_size=1, num_workers=0)\n",
    "    text = getattr(outputs[0], \"text\", outputs[0])\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_CACHE_DIR = Path('/home/sallundina/voice-image-editor/notebooks/audio_cache')\n",
    "\n",
    "def get_audio_path(sample_idx):\n",
    "    audio_file = AUDIO_CACHE_DIR / f\"sample_{sample_idx}.wav\"\n",
    "    return str(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reference (raw): Заставь птицу сложить крылья.\n",
      "Reference (normalized): заставь птицу сложить крылья\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WHISPER LARGE-V3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw transcription:  заставь птицу сложить крылья.\n",
      "Normalized transcription: заставь птицу сложить крылья\n",
      "WER: 0.0000\n",
      "CER: 0.0000\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WHISPER SMALL:\n",
      "Raw transcription:  застав птицу сложить крылья.\n",
      "Normalized transcription: застав птицу сложить крылья\n",
      "WER: 0.2500\n",
      "CER: 0.0357\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GIGAAM-V3:\n",
      "Raw transcription: заставь птицу сложить крылья.\n",
      "Normalized transcription: заставь птицу сложить крылья\n",
      "WER: 0.0000\n",
      "CER: 0.0000\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "T-ONE:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-12-07 07:04:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-07 07:04:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw transcription: заставь птицу сложить крылья\n",
      "Normalized transcription: заставь птицу сложить крылья\n",
      "WER: 0.0000\n",
      "CER: 0.0000\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "NVIDIA PARAKEET:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcribing: 1it [00:00,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw transcription: заставь птицу сложить крылья.\n",
      "Normalized transcription: заставь птицу сложить крылья\n",
      "WER: 0.0000\n",
      "CER: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим на один пример\n",
    "test_sample_idx = 1\n",
    "test_sample = eval_dataset[test_sample_idx]\n",
    "\n",
    "print(f\"\\nReference (raw): {test_sample['EDITING_INSTRUCTION_RU']}\")\n",
    "print(f\"Reference (normalized): {normalize_text(test_sample['EDITING_INSTRUCTION_RU'])}\")\n",
    "\n",
    "audio_path = get_audio_path(test_sample_idx)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 100)\n",
    "print(\"WHISPER LARGE-V3:\")\n",
    "whisper_large_processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v3\")\n",
    "whisper_large_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v3\").to(device)\n",
    "whisper_large_raw = transcribe_whisper(audio_path, processor=whisper_large_processor, model=whisper_large_model)\n",
    "whisper_large_norm = normalize_text(whisper_large_raw)\n",
    "print(f\"Raw transcription: {whisper_large_raw}\")\n",
    "print(f\"Normalized transcription: {whisper_large_norm}\")\n",
    "print(f\"WER: {wer(normalize_text(test_sample['EDITING_INSTRUCTION_RU']), whisper_large_norm):.4f}\")\n",
    "print(f\"CER: {cer(normalize_text(test_sample['EDITING_INSTRUCTION_RU']), whisper_large_norm):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 100)\n",
    "print(\"WHISPER SMALL:\")\n",
    "whisper_small_processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "whisper_small_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\").to(device)\n",
    "whisper_small_raw = transcribe_whisper(audio_path, processor=whisper_small_processor, model=whisper_small_model)\n",
    "whisper_small_norm = normalize_text(whisper_small_raw)\n",
    "print(f\"Raw transcription: {whisper_small_raw}\")\n",
    "print(f\"Normalized transcription: {whisper_small_norm}\")\n",
    "print(f\"WER: {wer(normalize_text(test_sample['EDITING_INSTRUCTION_RU']), whisper_small_norm):.4f}\")\n",
    "print(f\"CER: {cer(normalize_text(test_sample['EDITING_INSTRUCTION_RU']), whisper_small_norm):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 100)\n",
    "print(\"GIGAAM-V3:\")\n",
    "gigaam_raw = transcribe_gigaam(audio_path)\n",
    "gigaam_norm = normalize_text(gigaam_raw)\n",
    "print(f\"Raw transcription: {gigaam_raw}\")\n",
    "print(f\"Normalized transcription: {gigaam_norm}\")\n",
    "print(f\"WER: {wer(normalize_text(test_sample['EDITING_INSTRUCTION_RU']), gigaam_norm):.4f}\")\n",
    "print(f\"CER: {cer(normalize_text(test_sample['EDITING_INSTRUCTION_RU']), gigaam_norm):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 100)\n",
    "print(\"T-ONE:\")\n",
    "tone_raw = transcribe_tone(audio_path)\n",
    "tone_norm = normalize_text(tone_raw)\n",
    "print(f\"Raw transcription: {tone_raw}\")\n",
    "print(f\"Normalized transcription: {tone_norm}\")\n",
    "print(f\"WER: {wer(normalize_text(test_sample['EDITING_INSTRUCTION_RU']), tone_norm):.4f}\")\n",
    "print(f\"CER: {cer(normalize_text(test_sample['EDITING_INSTRUCTION_RU']), tone_norm):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 100)\n",
    "print(\"NVIDIA PARAKEET:\")\n",
    "parakeet_raw = transcribe_parakeet(audio_path)\n",
    "parakeet_norm = normalize_text(parakeet_raw)\n",
    "print(f\"Raw transcription: {parakeet_raw}\")\n",
    "print(f\"Normalized transcription: {parakeet_norm}\")\n",
    "print(f\"WER: {wer(normalize_text(test_sample['EDITING_INSTRUCTION_RU']), parakeet_norm):.4f}\")\n",
    "print(f\"CER: {cer(normalize_text(test_sample['EDITING_INSTRUCTION_RU']), parakeet_norm):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(dataset, transcribe_func, model_name, max_samples=None):\n",
    "    results = []\n",
    "    all_references = []\n",
    "    all_hypotheses = []\n",
    "    \n",
    "    num_samples = min(len(dataset), max_samples) if max_samples else len(dataset)\n",
    "    \n",
    "    print(f\"\\nEvaluating {model_name} on {num_samples} samples...\")\n",
    "    \n",
    "    for i in tqdm(range(num_samples), desc=f\"{model_name}\"):\n",
    "        try:\n",
    "            sample = dataset[i]\n",
    "            \n",
    "            reference_raw = sample['EDITING_INSTRUCTION_RU']\n",
    "            reference = normalize_text(reference_raw)\n",
    "            \n",
    "            audio_path = get_audio_path(i)\n",
    "            \n",
    "            hypothesis_raw = transcribe_func(audio_path)\n",
    "            hypothesis = normalize_text(hypothesis_raw)\n",
    "            \n",
    "            all_references.append(reference)\n",
    "            all_hypotheses.append(hypothesis)\n",
    "            \n",
    "            sample_wer = wer(reference, hypothesis)\n",
    "            sample_cer = cer(reference, hypothesis)\n",
    "            \n",
    "            results.append({\n",
    "                'sample_id': i,\n",
    "                'reference_raw': reference_raw,\n",
    "                'reference_normalized': reference,\n",
    "                'hypothesis_raw': hypothesis_raw,\n",
    "                'hypothesis_normalized': hypothesis,\n",
    "                'wer': sample_wer,\n",
    "                'cer': sample_cer\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing sample {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if len(all_references) == 0:\n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'wer': None,\n",
    "            'cer': None,\n",
    "            'num_samples': 0,\n",
    "            'results': []\n",
    "        }\n",
    "    \n",
    "    overall_wer = wer(all_references, all_hypotheses)\n",
    "    overall_cer = cer(all_references, all_hypotheses)\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'wer': overall_wer,\n",
    "        'cer': overall_cer,\n",
    "        'num_samples': len(all_references),\n",
    "        'results': results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SAMPLES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Whisper Large on 1000 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Whisper Large: 100%|██████████| 1000/1000 [09:25<00:00,  1.77it/s]\n"
     ]
    }
   ],
   "source": [
    "transcribe_whisper_small = create_whisper_transcriber(\"openai/whisper-large-v3\")\n",
    "results_whisper_large = evaluate_model(eval_dataset, transcribe_whisper_small, \"Whisper Large\", MAX_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Whisper Small on 1000 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Whisper Small: 100%|██████████| 1000/1000 [04:00<00:00,  4.15it/s]\n"
     ]
    }
   ],
   "source": [
    "transcribe_whisper_small = create_whisper_transcriber(\"openai/whisper-small\")\n",
    "results_whisper_small = evaluate_model(eval_dataset, transcribe_whisper_small, \"Whisper Small\", MAX_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating GigaAM-v3 on 1000 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GigaAM-v3: 100%|██████████| 1000/1000 [04:21<00:00,  3.83it/s]\n"
     ]
    }
   ],
   "source": [
    "results_gigaam = evaluate_model(eval_dataset, transcribe_gigaam, \"GigaAM-v3\", MAX_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating T-one on 1000 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T-one: 100%|██████████| 1000/1000 [21:40<00:00,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "results_tone = evaluate_model(eval_dataset, transcribe_tone, \"T-one\", MAX_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_parakeet = evaluate_model(eval_dataset, transcribe_parakeet, \"NVIDIA Parakeet\", MAX_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = [\n",
    "    {\n",
    "        'Model': results_whisper_large['model_name'],\n",
    "        'WER': results_whisper_large['wer'],\n",
    "        'CER': results_whisper_large['cer'],\n",
    "        'Samples': results_whisper_large['num_samples']\n",
    "    },\n",
    "    {\n",
    "        'Model': results_whisper_small['model_name'],\n",
    "        'WER': results_whisper_small['wer'],\n",
    "        'CER': results_whisper_small['cer'],\n",
    "        'Samples': results_whisper_small['num_samples']\n",
    "    },\n",
    "    {\n",
    "        'Model': results_gigaam['model_name'],\n",
    "        'WER': results_gigaam['wer'],\n",
    "        'CER': results_gigaam['cer'],\n",
    "        'Samples': results_gigaam['num_samples']\n",
    "    },\n",
    "    {\n",
    "        'Model': results_tone['model_name'],\n",
    "        'WER': results_tone['wer'],\n",
    "        'CER': results_tone['cer'],\n",
    "        'Samples': results_tone['num_samples']\n",
    "    },\n",
    "    {\n",
    "        'Model': results_parakeet['model_name'],\n",
    "        'WER': results_parakeet['wer'],\n",
    "        'CER': results_parakeet['cer'],\n",
    "        'Samples': results_parakeet['num_samples']\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame(summary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Model      WER      CER  Samples\n",
      "  Whisper Large 0.066721 0.024128     1000\n",
      "  Whisper Small 0.108343 0.034571     1000\n",
      "      GigaAM-v3 0.052747 0.015834     1000\n",
      "          T-one 0.061060 0.017139     1000\n",
      "NVIDIA Parakeet 0.060302 0.016288     1000\n"
     ]
    }
   ],
   "source": [
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лидер - GigaAM-v3, но результаты достаточно близкие для всего кроме Whisper-small. Но с учетом размера (Whisper-large и Parakeet больше) и скорости инференса (T-one сильно медленнее) и примеров, которые смотрели глазами, остановили свой выбор на GigaAM-v3. Также GigaAM-v3 возвращает текст с правильной пунктуацией, капитализацией и тд, что очень удобно."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_new3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
