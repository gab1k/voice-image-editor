{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q lpips scikit-image opencv-python Pillow\n",
        "!pip install -q transformers torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_QZVloKIvdc",
        "outputId": "ce169365-96c7-4772-b6c0-2803a4ca2fa3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Reference-Based Evaluation\n",
        "Сравнивает генерации с ground truth эталоном\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import warnings\n",
        "import lpips\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class LightweightLPIPS:\n",
        "    def __init__(self, net='alex', use_gpu=None):\n",
        "\n",
        "\n",
        "        if use_gpu is None:\n",
        "            use_gpu = torch.cuda.is_available()\n",
        "\n",
        "        self.device = 'cuda' if use_gpu else 'cpu'\n",
        "\n",
        "        print(f\"Loading LPIPS ({net})...\")\n",
        "\n",
        "\n",
        "        self.model = lpips.LPIPS(net=net, verbose=False)\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "        if self.device == 'cuda':\n",
        "            allocated = torch.cuda.memory_allocated() / 1024**2\n",
        "            print(f\"   GPU Memory: {allocated:.1f} MB\")\n",
        "\n",
        "    def compute(self, img1: Image.Image, img2: Image.Image) -> float:\n",
        "\n",
        "        tensor1 = self._preprocess(img1)\n",
        "        tensor2 = self._preprocess(img2)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            distance = self.model(tensor1, tensor2)\n",
        "\n",
        "        similarity = 1.0 - distance.item()\n",
        "\n",
        "\n",
        "        if self.device == 'cuda':\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        return max(0.0, min(1.0, similarity))\n",
        "\n",
        "    def _preprocess(self, img: Image.Image) -> torch.Tensor:\n",
        "\n",
        "        max_size = 512\n",
        "        if max(img.size) > max_size:\n",
        "            ratio = max_size / max(img.size)\n",
        "            new_size = tuple(int(dim * ratio) for dim in img.size)\n",
        "            img = img.resize(new_size, Image.LANCZOS)\n",
        "\n",
        "        img_array = np.array(img).astype(np.float32) / 255.0\n",
        "        tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)\n",
        "\n",
        "\n",
        "        tensor = tensor * 2 - 1\n",
        "\n",
        "        return tensor.to(self.device)\n",
        "\n",
        "    def __del__(self):\n",
        "\n",
        "        if hasattr(self, 'device') and self.device == 'cuda':\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "class ReferenceBasedEvaluator:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        use_lpips: bool = True,\n",
        "        use_ssim: bool = True,\n",
        "        use_clip: bool = False,  # По умолчанию выключен для экономии памяти\n",
        "        lpips_net: str = 'alex',\n",
        "        device: str = None\n",
        "    ):\n",
        "\n",
        "        if device is None:\n",
        "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "        print(\"Initializing Reference-Based Evaluator\")\n",
        "        print(f\"Device: {self.device}\")\n",
        "        print(f\"LPIPS: {use_lpips}\")\n",
        "        print(f\"SSIM: {use_ssim}\")\n",
        "        print(f\"CLIP: {use_clip}\")\n",
        "        print()\n",
        "\n",
        "        self.lpips_model = None\n",
        "        self.clip_model = None\n",
        "        self.use_ssim = use_ssim\n",
        "\n",
        "\n",
        "        if use_lpips:\n",
        "            try:\n",
        "                self.lpips_model = LightweightLPIPS(\n",
        "                    net=lpips_net,\n",
        "                    use_gpu=(self.device == 'cuda')\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\" LPIPS loading failed: {e}\")\n",
        "\n",
        "        if use_clip and self.device == 'cuda':\n",
        "            try:\n",
        "\n",
        "                from transformers import CLIPModel, CLIPProcessor\n",
        "\n",
        "                self.clip_model = CLIPModel.from_pretrained(\n",
        "                    \"openai/clip-vit-base-patch32\"\n",
        "                ).to(self.device)\n",
        "                self.clip_processor = CLIPProcessor.from_pretrained(\n",
        "                    \"openai/clip-vit-base-patch32\"\n",
        "                )\n",
        "                self.clip_model.eval()\n",
        "\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"CLIP loading failed: {e}\")\n",
        "                self.clip_model = None\n",
        "\n",
        "        print(\" Evaluator initialized\")\n",
        "\n",
        "\n",
        "    def compute_lpips(\n",
        "        self,\n",
        "        generated: Image.Image,\n",
        "        ground_truth: Image.Image\n",
        "    ) -> float:\n",
        "\n",
        "        if self.lpips_model is None:\n",
        "            return 0.0\n",
        "\n",
        "        try:\n",
        "            return self.lpips_model.compute(generated, ground_truth)\n",
        "        except Exception as e:\n",
        "            print(f\" LPIPS computation failed: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "\n",
        "    def compute_ssim(\n",
        "        self,\n",
        "        generated: Image.Image,\n",
        "        ground_truth: Image.Image\n",
        "    ) -> float:\n",
        "\n",
        "        if not self.use_ssim:\n",
        "            return 0.0\n",
        "\n",
        "        try:\n",
        "            from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "\n",
        "            gen_array = np.array(generated.convert('RGB'))\n",
        "            gt_array = np.array(ground_truth.convert('RGB'))\n",
        "\n",
        "\n",
        "            if gen_array.shape != gt_array.shape:\n",
        "                from skimage.transform import resize\n",
        "                gen_array = resize(gen_array, gt_array.shape, anti_aliasing=True)\n",
        "                gen_array = (gen_array * 255).astype(np.uint8)\n",
        "\n",
        "\n",
        "            score = ssim(\n",
        "                gen_array,\n",
        "                gt_array,\n",
        "                channel_axis=2,\n",
        "                data_range=255\n",
        "            )\n",
        "\n",
        "            return float(score)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠SSIM computation failed: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    def compute_psnr(\n",
        "        self,\n",
        "        generated: Image.Image,\n",
        "        ground_truth: Image.Image\n",
        "    ) -> float:\n",
        "\n",
        "        try:\n",
        "            gen_array = np.array(generated.convert('RGB')).astype(np.float64)\n",
        "            gt_array = np.array(ground_truth.convert('RGB')).astype(np.float64)\n",
        "\n",
        "\n",
        "            if gen_array.shape != gt_array.shape:\n",
        "                from skimage.transform import resize\n",
        "                gen_array = resize(gen_array, gt_array.shape, anti_aliasing=True)\n",
        "                gen_array = gen_array * 255\n",
        "\n",
        "            mse = np.mean((gen_array - gt_array) ** 2)\n",
        "\n",
        "            if mse == 0:\n",
        "                return 100.0\n",
        "\n",
        "            max_pixel = 255.0\n",
        "            psnr = 20 * np.log10(max_pixel / np.sqrt(mse))\n",
        "\n",
        "            return float(psnr)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" PSNR computation failed: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "\n",
        "    def compute_clip_similarity(\n",
        "        self,\n",
        "        generated: Image.Image,\n",
        "        ground_truth: Image.Image\n",
        "    ) -> float:\n",
        "\n",
        "        if self.clip_model is None:\n",
        "            return 0.0\n",
        "\n",
        "        try:\n",
        "            inputs = self.clip_processor(\n",
        "                images=[generated, ground_truth],\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                image_features = self.clip_model.get_image_features(**inputs)\n",
        "\n",
        "                # Normalize\n",
        "                image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "                # Cosine similarity\n",
        "                similarity = torch.cosine_similarity(\n",
        "                    image_features[0:1],\n",
        "                    image_features[1:2]\n",
        "                ).item()\n",
        "\n",
        "            # Cleanup\n",
        "            if self.device == 'cuda':\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            return float(similarity)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" CLIP computation failed: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "\n",
        "    def compute_color_similarity(\n",
        "        self,\n",
        "        generated: Image.Image,\n",
        "        ground_truth: Image.Image\n",
        "    ) -> float:\n",
        "\n",
        "        try:\n",
        "            import cv2\n",
        "\n",
        "            gen_array = np.array(generated.convert('RGB'))\n",
        "            gt_array = np.array(ground_truth.convert('RGB'))\n",
        "\n",
        "\n",
        "            if gen_array.shape != gt_array.shape:\n",
        "                gen_array = cv2.resize(gen_array, (gt_array.shape[1], gt_array.shape[0]))\n",
        "\n",
        "\n",
        "            hist_gen = cv2.calcHist(\n",
        "                [gen_array], [0, 1, 2], None,\n",
        "                [8, 8, 8], [0, 256, 0, 256, 0, 256]\n",
        "            )\n",
        "            hist_gt = cv2.calcHist(\n",
        "                [gt_array], [0, 1, 2], None,\n",
        "                [8, 8, 8], [0, 256, 0, 256, 0, 256]\n",
        "            )\n",
        "\n",
        "\n",
        "            hist_gen = cv2.normalize(hist_gen, hist_gen).flatten()\n",
        "            hist_gt = cv2.normalize(hist_gt, hist_gt).flatten()\n",
        "\n",
        "\n",
        "            correlation = cv2.compareHist(\n",
        "                hist_gen.reshape(-1, 1),\n",
        "                hist_gt.reshape(-1, 1),\n",
        "                cv2.HISTCMP_CORREL\n",
        "            )\n",
        "\n",
        "            return float(correlation)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  Color similarity computation failed: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "\n",
        "\n",
        "    def evaluate_single(\n",
        "        self,\n",
        "        generated: Image.Image,\n",
        "        ground_truth: Image.Image,\n",
        "        weights: Optional[Dict[str, float]] = None,\n",
        "        verbose: bool = False\n",
        "    ) -> Dict[str, float]:\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Evaluating...\", end=\" \")\n",
        "\n",
        "        results = {}\n",
        "\n",
        "\n",
        "        if weights is None:\n",
        "            weights = {\n",
        "                'lpips': 0.50,\n",
        "                'ssim': 0.30,\n",
        "                'clip': 0.0,\n",
        "                'color': 0.20\n",
        "            }\n",
        "\n",
        "        if verbose:\n",
        "            print(\"LPIPS...\", end=\" \")\n",
        "        results['lpips_similarity'] = self.compute_lpips(generated, ground_truth)\n",
        "\n",
        "\n",
        "        if verbose:\n",
        "            print(\"SSIM...\", end=\" \")\n",
        "        results['ssim'] = self.compute_ssim(generated, ground_truth)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"PSNR...\", end=\" \")\n",
        "        results['psnr'] = self.compute_psnr(generated, ground_truth)\n",
        "\n",
        "\n",
        "        if self.clip_model is not None:\n",
        "            if verbose:\n",
        "                print(\"CLIP...\", end=\" \")\n",
        "            results['clip_similarity'] = self.compute_clip_similarity(\n",
        "                generated, ground_truth\n",
        "            )\n",
        "\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Color...\", end=\" \")\n",
        "        results['color_similarity'] = self.compute_color_similarity(\n",
        "            generated, ground_truth\n",
        "        )\n",
        "\n",
        "\n",
        "        combined = 0.0\n",
        "        total_weight = 0.0\n",
        "\n",
        "        for metric, weight in weights.items():\n",
        "            metric_key = f'{metric}_similarity' if metric != 'ssim' else metric\n",
        "\n",
        "            if metric_key in results:\n",
        "                value = results[metric_key]\n",
        "\n",
        "\n",
        "                if metric == 'psnr':\n",
        "                    value = min(1.0, max(0.0, (value - 20) / 20))\n",
        "\n",
        "                combined += value * weight\n",
        "                total_weight += weight\n",
        "\n",
        "        results['combined_score'] = combined / total_weight if total_weight > 0 else 0.0\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Done! Score: {results['combined_score']:.4f}\")\n",
        "\n",
        "        return results\n",
        ""
      ],
      "metadata": {
        "id": "bJJkLnXqYGsW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    def evaluate_batch(\n",
        "        self,\n",
        "        generated_images: List[Image.Image],\n",
        "        ground_truth: Image.Image,\n",
        "        model_names: Optional[List[str]] = None,\n",
        "        verbose: bool = True\n",
        "    ) -> List[Dict[str, any]]:\n",
        "\n",
        "        if model_names is None:\n",
        "            model_names = [f\"Model_{i+1}\" for i in range(len(generated_images))]\n",
        "\n",
        "        results = []\n",
        "\n",
        "        for i, (gen_img, model_name) in enumerate(zip(generated_images, model_names)):\n",
        "            if verbose:\n",
        "                print(f\"\\n[{i+1}/{len(generated_images)}] {model_name}: \", end=\"\")\n",
        "\n",
        "            scores = self.evaluate_single(gen_img, ground_truth, verbose=verbose)\n",
        "            scores['model_name'] = model_name\n",
        "            scores['model_index'] = i\n",
        "\n",
        "            results.append(scores)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def select_best(\n",
        "        self,\n",
        "        generated_images: List[Image.Image],\n",
        "        ground_truth: Image.Image,\n",
        "        model_names: Optional[List[str]] = None,\n",
        "        metric: str = 'combined_score'\n",
        "    ) -> Tuple[int, Dict[str, any]]:\n",
        "        \"\"\"\n",
        "        Выбирает лучшую генерацию\n",
        "\n",
        "        Returns:\n",
        "            (best_index, best_scores)\n",
        "        \"\"\"\n",
        "        results = self.evaluate_batch(generated_images, ground_truth, model_names)\n",
        "\n",
        "        best_idx = max(range(len(results)), key=lambda i: results[i].get(metric, 0))\n",
        "\n",
        "        return best_idx, results[best_idx]\n",
        "\n"
      ],
      "metadata": {
        "id": "BXMEr2VXJoT0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = ReferenceBasedEvaluator(\n",
        "        use_lpips=True,\n",
        "        use_ssim=True,\n",
        "        use_clip=False,\n",
        "        lpips_net='alex'\n",
        ")\n",
        "\n",
        "\n",
        "ground_truth = Image.open('/content/ground_truth.jpg')\n",
        "generated = Image.open('/content/model_output.jpg')\n",
        "\n",
        "scores = evaluator.evaluate_single(generated, ground_truth, verbose=True)\n",
        "print(f\"Combined Score: {scores['combined_score']:.4f}\")\n",
        "print(f\"LPIPS: {scores['lpips_similarity']:.4f}\")\n",
        "print(f\"SSIM: {scores['ssim']:.4f}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDl6I1x6YOVb",
        "outputId": "51902d1b-61ac-4b9d-98d9-b3d530cdd182"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Reference-Based Evaluator\n",
            "Device: cpu\n",
            "LPIPS: True\n",
            "SSIM: True\n",
            "CLIP: False\n",
            "\n",
            "Loading LPIPS (alex)...\n",
            " Evaluator initialized\n",
            "Evaluating... LPIPS... SSIM... PSNR... Color... Done! Score: 0.6427\n",
            "Combined Score: 0.6427\n",
            "LPIPS: 0.6753\n",
            "SSIM: 0.6417\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = ReferenceBasedEvaluator(\n",
        "        use_lpips=True,\n",
        "        use_ssim=True,\n",
        "        use_clip=True,\n",
        "        lpips_net='alex'\n",
        ")\n",
        "\n",
        "\n",
        "ground_truth = Image.open('/content/ground_truth.jpg')\n",
        "generated = Image.open('/content/model_output.jpg')\n",
        "\n",
        "scores = evaluator.evaluate_single(generated, ground_truth, verbose=True)\n",
        "print(f\"Combined Score: {scores['combined_score']:.4f}\")\n",
        "print(f\"LPIPS: {scores['lpips_similarity']:.4f}\")\n",
        "print(f\"SSIM: {scores['ssim']:.4f}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5X6WDbsAPCl5",
        "outputId": "6b5dd5f3-8de5-457e-a7bd-91587c121667"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Reference-Based Evaluator\n",
            "Device: cpu\n",
            "LPIPS: True\n",
            "SSIM: True\n",
            "CLIP: True\n",
            "\n",
            "Loading LPIPS (alex)...\n",
            " Evaluator initialized\n",
            "Evaluating... LPIPS... SSIM... PSNR... Color... Done! Score: 0.6427\n",
            "Combined Score: 0.6427\n",
            "LPIPS: 0.6753\n",
            "SSIM: 0.6417\n"
          ]
        }
      ]
    }
  ]
}